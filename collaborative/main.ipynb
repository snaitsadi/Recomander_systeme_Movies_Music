{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63265bf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Preamble\n",
    "Various global variables, parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90bea9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "DATASET_MAX_SIZE = 1_000_000\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f00813",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This work is using the Million Song Dataset, specifically the one with user listenings count.\n",
    "It's available at http://millionsongdataset.com/tasteprofile/.\n",
    "> Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, & Paul Lamere (2011). The Million Song Dataset. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011).\n",
    "\n",
    "It consists of a huge list (48m+ entries) of triplets `(user id, song id, listnings count)`.\n",
    "\n",
    "## Dataset parsing\n",
    "\n",
    "Read from the text file, simple parse and convert to numpy data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bd3ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43db01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Maps users and songs to their unique index for further referencing as matrix index\n",
    "USER_MAPPING: dict[str, int] = {}\n",
    "SONG_MAPPING: dict[str, int] = {}\n",
    "\n",
    "# It's a list of tuples (user, song, listening count)\n",
    "dataset_triplet_dtype = np.dtype(\n",
    "    [\n",
    "        (\"User index\", np.uint32),\n",
    "        (\"Song index\", np.uint32),\n",
    "        (\"Listening count\", np.float64),\n",
    "    ]\n",
    ")\n",
    "dataset_raw: list[tuple[int, int, int]] = []\n",
    "with open(\"../train_triplets.txt\", \"r\") as dataset_file:\n",
    "    for line in dataset_file:\n",
    "        user_id, song_id, listening_count = line.split(\"\\t\")\n",
    "\n",
    "        dataset_raw.append(\n",
    "            (\n",
    "                USER_MAPPING.setdefault(user_id, len(USER_MAPPING)),\n",
    "                SONG_MAPPING.setdefault(song_id, len(SONG_MAPPING)),\n",
    "                int(listening_count),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if len(dataset_raw) >= DATASET_MAX_SIZE:\n",
    "            break\n",
    "\n",
    "dataset = np.array(dataset_raw, dtype=dataset_triplet_dtype)\n",
    "\n",
    "print(dataset.dtype)\n",
    "print(dataset)\n",
    "print(\n",
    "    f\"Parsed {len(SONG_MAPPING)} and {len(USER_MAPPING)} users, for a total of {len(dataset)} triplets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48833e22",
   "metadata": {},
   "source": [
    "## Dataset normalization\n",
    "\n",
    "Standardize: remove mean and divide by standard deviation.\n",
    "Hence, values are unitless, centered around 0, and spans in $[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2c5e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"Listening count\"] = (\n",
    "    dataset[\"Listening count\"] - dataset[\"Listening count\"].mean()\n",
    ") / dataset[\"Listening count\"].std()\n",
    "\n",
    "print(dataset.dtype, dataset.shape)\n",
    "print(dataset[\"Listening count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c09b7f",
   "metadata": {},
   "source": [
    "## Dataset training/validation preparation\n",
    "\n",
    "Shuffle & split into subsets.\n",
    "\n",
    "We take 2/3 for the training, and 1/3 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585b18c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_set_size = int(len(dataset) * 0.66)\n",
    "\n",
    "dataset_perm = np.random.permutation(len(dataset))\n",
    "dataset_shuffled = dataset[dataset_perm]\n",
    "\n",
    "train_set = dataset_shuffled[:training_set_size]\n",
    "validation_set = dataset_shuffled[training_set_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27b824",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "The method used is a Stochastic Gradient Descent (SGD), using Regularized Mean Squared Error (RMSE) as the loss function.\n",
    "It corresponds to\n",
    "$$\n",
    "\\min_{q^*,p^*} \\sum_{(u,i) \\in \\mathcal{K}} \\left(r_{ui} - q_i^Tp_u\\right)^2 + \\lambda\\left(||q_i||^2 + ||p_u||^2\\right)\n",
    "$$\n",
    "\n",
    "The overall method is taken from [Matrix Factorization Techniques for Recommender Systems\n",
    "](https://ieeexplore.ieee.org/document/5197422).\n",
    "> Y. Koren, R. Bell and C. Volinsky, \"Matrix Factorization Techniques for Recommender Systems,\" in Computer, vol. 42, no. 8, pp. 30-37, Aug. 2009, doi: 10.1109/MC.2009.263. keywords: {Recommender systems;Motion pictures;Filtering;Collaboration;Sea measurements;Predictive models;Genomics;Bioinformatics;Nearest neighbor searches;Computational intelligence;Netflix Prize;Matrix factorization},\n",
    "\n",
    "\n",
    "\n",
    "## Prepare learning\n",
    "\n",
    "Prepare learning sets $q$ and $p$, which are random matrices of shapes $(|\\mathrm{songs}|, l)$ and $(|\\mathrm{users}|, l)$.\n",
    "\n",
    "Set parameters:\n",
    "- Size `l` of latent space to embed users & films\n",
    "- Learning rate $\\gamma$\n",
    "- Regularization $\\lambda$\n",
    "- Number of epochs (rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635d091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# (Hyperparameter) Size of latent space to make the embeddings\n",
    "l = 100\n",
    "# Initial (random) values\n",
    "# Shape: (#SONGS, l)\n",
    "q = np.random.random_sample((len(SONG_MAPPING), l))\n",
    "if DEBUG:\n",
    "    print(q.shape, q.dtype, q)\n",
    "# Shape: (#USERS, l)\n",
    "p = np.random.random_sample((len(USER_MAPPING), l))\n",
    "if DEBUG:\n",
    "    print(p.shape, p.dtype, p)\n",
    "\n",
    "average_listening_count = train_set[\"Listening count\"].mean()\n",
    "\n",
    "b_song = np.random.random_sample(len(SONG_MAPPING))\n",
    "if DEBUG:\n",
    "    print(b_song.shape, b_song.dtype, b_song)\n",
    "# Shape: (#USERS, l)\n",
    "b_user = np.random.random_sample(len(USER_MAPPING))\n",
    "if DEBUG:\n",
    "    print(b_user.shape, b_user.dtype, b_user)\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lbd = 0.001\n",
    "gamma = 0.0005\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160353f",
   "metadata": {},
   "source": [
    "## Actual learning\n",
    "\n",
    "Process the SGD, accumulating loss so it can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1998652",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from typing import TypedDict, cast\n",
    "\n",
    "\n",
    "class LearningStats(TypedDict):\n",
    "    \"\"\"\n",
    "    Learning stats (losses: train & validation) for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    losses_train: list[np.float64 | float]\n",
    "    losses_validation: list[np.float64 | float]\n",
    "    accuracy_train: list[np.float64 | float]\n",
    "    accuracy_validation: list[np.float64 | float]\n",
    "\n",
    "\n",
    "def train(l: int, lbd: float, gamma: float, n_epochs: int) -> LearningStats:\n",
    "    learning_stats: LearningStats = {\n",
    "        \"losses_train\": [np.nan] * n_epochs,\n",
    "        \"losses_validation\": [np.nan] * n_epochs,\n",
    "        \"accuracy_train\": [np.nan] * n_epochs,\n",
    "        \"accuracy_validation\": [np.nan] * n_epochs,\n",
    "    }\n",
    "\n",
    "    print(f\"Training with l={l}, lambda={lbd}, gamma={gamma} for {n_epochs} epochs.\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        loss_sum: float = 0\n",
    "        accuracy_sum: float = 0\n",
    "\n",
    "        np.random.shuffle(train_set)  # Reorder each epoch\n",
    "        # user \\in [0, #USERS - 1]\n",
    "        # song \\in [0, #SONGS - 1]\n",
    "        # listenings \\in N (r_ui, \"true\" value)\n",
    "        for i, (user, song, listening_count) in enumerate(\n",
    "            cast(Iterable[tuple[np.uint32, np.uint32, np.float64]], train_set)\n",
    "        ):\n",
    "            if DEBUG:\n",
    "                print(\n",
    "                    f\"Training value {i}/{len(train_set)}: ({user},{song},{listening_count})\"\n",
    "                )\n",
    "\n",
    "            # Predicted value\n",
    "            p_u = p[user].copy()\n",
    "            q_i = q[song].copy()\n",
    "            b_u = b_user[user].copy()\n",
    "            b_i = b_song[song].copy()\n",
    "\n",
    "            listenings_hat = (p_u.T @ q_i) + average_listening_count + b_u + b_i\n",
    "\n",
    "            # Prediction error\n",
    "            e_ui = listening_count - listenings_hat\n",
    "\n",
    "            # This is the learning part\n",
    "            q[song] += gamma * (e_ui * p_u - lbd * q_i)\n",
    "            p[user] += gamma * (e_ui * q_i - lbd * p_u)\n",
    "            b_user[user] += gamma * (e_ui - lbd * b_u)\n",
    "            b_song[song] += gamma * (e_ui - lbd * b_i)\n",
    "\n",
    "            # Loss OUTDATED\n",
    "            loss = e_ui**2 + lbd * (np.linalg.norm(q_i) ** 2 + np.linalg.norm(p_u) ** 2)\n",
    "            loss_sum += loss\n",
    "\n",
    "            # Accuracy\n",
    "            accuracy = e_ui**2\n",
    "            accuracy_sum += accuracy\n",
    "\n",
    "        learning_stats[\"losses_train\"][epoch] = loss_sum\n",
    "        learning_stats[\"accuracy_train\"][epoch] = np.sqrt(accuracy_sum / len(train_set))\n",
    "\n",
    "        # Now evaluating on validation data\n",
    "        loss_validation_sum = 0\n",
    "        accuracy_validation_sum = 0\n",
    "        for user, song, listening_count in validation_set:\n",
    "            listenings_hat = p[user].T @ q[song] + average_listening_count + b_user[user] + b_song[song]\n",
    "\n",
    "            e_ui = listening_count - listenings_hat\n",
    "\n",
    "            # Loss\n",
    "            loss = e_ui**2 + lbd * (\n",
    "                np.linalg.norm(q[song]) ** 2 + np.linalg.norm(p[user]) ** 2\n",
    "            )\n",
    "            loss_validation_sum += loss\n",
    "\n",
    "            # Accuracy\n",
    "            accuracy = e_ui**2\n",
    "            accuracy_validation_sum += accuracy\n",
    "\n",
    "        learning_stats[\"losses_validation\"][epoch] = loss_validation_sum\n",
    "        learning_stats[\"accuracy_validation\"][epoch] = np.sqrt(\n",
    "            accuracy_validation_sum / len(validation_set)\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Loss (train): {learning_stats['losses_train'][epoch]}, loss (validation): {learning_stats['losses_validation'][epoch]}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Accuracy (train): {learning_stats['accuracy_train'][epoch]}, Accuracy (validation): {learning_stats['accuracy_validation'][epoch]}\"\n",
    "        )\n",
    "\n",
    "    return learning_stats\n",
    "\n",
    "\n",
    "training_stats = train(l, lbd, gamma, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59952145",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Learning results\n",
    "\n",
    "We first analyze the learning raw results: training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412bef4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Loss\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "_ = ax.plot(training_stats[\"losses_train\"], label=\"Train loss\")\n",
    "_ = ax.plot(training_stats[\"losses_validation\"], label=\"Validation loss\")\n",
    "_ = ax.set_yscale(\"log\")\n",
    "_ = ax.set_xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "_ = ax.set_ylabel(\"Loss\")\n",
    "_ = ax.set_title(\"Losses during learning\")\n",
    "_ = fig.legend()\n",
    "\n",
    "# Accuracy\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "_ = ax.plot(training_stats[\"accuracy_train\"], label=\"Train accuracy\")\n",
    "_ = ax.plot(training_stats[\"accuracy_validation\"], label=\"Validation accuracy\")\n",
    "_ = ax.set_yscale(\"log\")\n",
    "_ = ax.set_xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "_ = ax.set_ylabel(\"Accuracy (RMSE)\")\n",
    "_ = ax.set_title(\"Accuracy during learning\")\n",
    "_ = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a641d",
   "metadata": {},
   "source": [
    "We analyze the differents losses depending on the value of L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed341548",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "losses_train: list[float] = []\n",
    "losses_validation: list[float] = []\n",
    "accuracies_train: list[float] = []\n",
    "accuracies_validation: list[float] = []\n",
    "l_values = [20, 30, 40, 50, 60, 70]\n",
    "n_epochs = 10\n",
    "\n",
    "training_set_size = int(len(dataset) * 0.66)\n",
    "\n",
    "dataset_perm = np.random.permutation(len(dataset))\n",
    "dataset_shuffled = dataset[dataset_perm]\n",
    "\n",
    "train_set = dataset_shuffled[:training_set_size]\n",
    "validation_set = dataset_shuffled[training_set_size:]\n",
    "\n",
    "for l in l_values:\n",
    "    print(f\"Training for l={l}\")\n",
    "    q = np.random.random_sample((len(SONG_MAPPING), l))\n",
    "    p = np.random.random_sample((len(USER_MAPPING), l))\n",
    "\n",
    "    LearningStats = train(l, lbd, gamma*10, n_epochs)\n",
    "    losses_train.append(LearningStats[\"losses_train\"][-1])\n",
    "    losses_validation.append(LearningStats[\"losses_validation\"][-1])\n",
    "    accuracies_train.append(LearningStats[\"accuracy_train\"][-1])\n",
    "    accuracies_validation.append(LearningStats[\"accuracy_validation\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a16abe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "figL, ax = plt.subplots(figsize=(10, 5))\n",
    "_ = ax.set_title(\"Losses depending on L\")\n",
    "_ = ax.set_xlabel(\"L\")\n",
    "_ = ax.set_ylabel(\"Loss\")\n",
    "_ = ax.plot(l_values, losses_train, label=\"Train loss\")\n",
    "_ = ax.plot(l_values, losses_validation, label=\"Validation loss\")\n",
    "_ = figL.legend()\n",
    "plt.close(figL)\n",
    "\n",
    "figL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47733b2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "figA, ax = plt.subplots(figsize=(10, 5))\n",
    "_ = ax.set_title(\"Accuracy depending on L\")\n",
    "_ = ax.set_xlabel(\"L\")\n",
    "_ = ax.set_ylabel(\"Accuracy\")\n",
    "_ = ax.plot(l_values, accuracies_train, label=\"Train accuracy\")\n",
    "_ = ax.plot(l_values, accuracies_validation, label=\"Validation accuracy\")\n",
    "_ = figA.legend()\n",
    "plt.close(figA)\n",
    "\n",
    "figA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5f1cc",
   "metadata": {},
   "source": [
    "## ... more analysis\n",
    "\n",
    "Evaluation of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a372d",
   "metadata": {},
   "source": [
    "# Offline metrics\n",
    "## Background of a few classical metrics\n",
    "### Precision\n",
    "relevant retrived instances / all retrieved instances\n",
    "### Recall\n",
    "relevant retrived instances / all relevant instances\n",
    "## Variant used\n",
    "For our use case, in order to have a metric that works across content based and collaborative filtering.\n",
    "\n",
    "It is intuitive to use Precision@k and Recall@k.\n",
    "\n",
    "In our case for example Precision_predict_music(user, k) would be\n",
    "\n",
    "Precision_predict_music@k = the actual number of top k favorite songs of a user in the k predicted songs / the k favorite songs of a user that the model predict (true positive / predicted positive)\n",
    "\n",
    "And another example Recall_predict_user(music, k)\n",
    "\n",
    "Recall_predict_user@k = the actual number of top k lovers of the song in the k predicted users / the k users that like the song the most (true positive / true positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd40e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming that the model matrices are accessible\n",
    "def user_favorite(user_id, k):\n",
    "    \"\"\"Return k indices of favorite songs of a user\"\"\"\n",
    "    user_index = USER_MAPPING[user_id]\n",
    "    \n",
    "    # Filter dataset to get only this user's listening records\n",
    "    user_records = dataset[dataset[\"User index\"] == user_index]\n",
    "    \n",
    "    # Sort by listening count in descending order\n",
    "    sorted_records = np.sort(user_records, order=\"Listening count\")[::-1]\n",
    "    \n",
    "    # Return the top k song indices\n",
    "    return sorted_records[\"Song index\"][:k]\n",
    "\n",
    "def music_top_fans(song_id, k):\n",
    "    \"\"\"Return k indices of biggest fans of a song\"\"\"\n",
    "    song_index = SONG_MAPPING[song_id]\n",
    "    \n",
    "    # Filter dataset to get only this song's listening records\n",
    "    song_records = dataset[dataset[\"Song index\"] == song_index]\n",
    "    \n",
    "    # Sort by listening count in descending order\n",
    "    sorted_records = np.sort(song_records, order=\"Listening count\")[::-1]\n",
    "    \n",
    "    # Return the top k user indices (the biggest fans of this song)\n",
    "    return sorted_records[\"User index\"][:k]\n",
    "    \n",
    "def predict_favorite_music(user_id, k):\n",
    "    user_index = USER_MAPPING[user_id]\n",
    "    predicted_scores = p[user_index] @ q.T  # Shape: (num_songs,)\n",
    "    \n",
    "    # Get indices that would sort scores in descending order\n",
    "    top_song_indices = np.argsort(predicted_scores)[::-1]\n",
    "    \n",
    "    return top_song_indices[:k]\n",
    "\n",
    "def predict_top_fans(song_id, k):\n",
    "    song_index = SONG_MAPPING[song_id]\n",
    "    predicted_scores = p @ q[song_index]  # Shape: (num_users,)\n",
    "    \n",
    "    # Get indices that would sort scores in descending order\n",
    "    top_user_indices = np.argsort(predicted_scores)[::-1]\n",
    "    \n",
    "    return top_user_indices[:k]\n",
    "\n",
    "def precision_recall_predict_music(user_id, k):\n",
    "    # Get actual favorites (ground truth)\n",
    "    actual_top_k = set(user_favorite(user_id, k))\n",
    "    \n",
    "    # Get predicted favorites\n",
    "    predicted_top_k = set(predict_favorite_music(user_id, k))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    true_positives = len(actual_top_k & predicted_top_k)\n",
    "    \n",
    "    precision = true_positives / k if k > 0 else 0\n",
    "    recall = true_positives / len(actual_top_k) if len(actual_top_k) > 0 else 0\n",
    "    \n",
    "    return (precision, recall)\n",
    "\n",
    "def precision_recall_predict_users(song_id, k):\n",
    "    # Get actual top fans (ground truth)\n",
    "    actual_top_k = set(music_top_fans(song_id, k))\n",
    "    \n",
    "    # Get predicted top fans\n",
    "    predicted_top_k = set(predict_top_fans(song_id, k))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    true_positives = len(actual_top_k & predicted_top_k)\n",
    "    \n",
    "    precision = true_positives / k if k > 0 else 0\n",
    "    recall = true_positives / len(actual_top_k) if len(actual_top_k) > 0 else 0\n",
    "    \n",
    "    return (precision, recall)\n",
    "\n",
    "def precision_recall_predict_music_all(k):\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    num_users = len(USER_MAPPING)\n",
    "    \n",
    "    for user_id in USER_MAPPING:\n",
    "        # Get actual favorites (ground truth)\n",
    "        actual_top_k = set(user_favorite(user_id, k))\n",
    "        \n",
    "        # Get predicted favorites\n",
    "        predicted_top_k = set(predict_favorite_music(user_id, k))\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        true_positives = len(actual_top_k & predicted_top_k)\n",
    "        \n",
    "        precision = true_positives / k if k > 0 else 0\n",
    "        recall = true_positives / len(actual_top_k) if len(actual_top_k) > 0 else 0\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "    \n",
    "    # Return average precision and recall across all users\n",
    "    avg_precision = total_precision / num_users if num_users > 0 else 0\n",
    "    avg_recall = total_recall / num_users if num_users > 0 else 0\n",
    "    \n",
    "    return (avg_precision, avg_recall)\n",
    "\n",
    "\n",
    "def precision_recall_predict_users_all(k):\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    num_songs = len(SONG_MAPPING)\n",
    "    \n",
    "    for song_id in SONG_MAPPING:\n",
    "        # Get actual top fans (ground truth)\n",
    "        actual_top_k = set(music_top_fans(song_id, k))\n",
    "        \n",
    "        # Get predicted top fans\n",
    "        predicted_top_k = set(predict_top_fans(song_id, k))\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        true_positives = len(actual_top_k & predicted_top_k)\n",
    "        \n",
    "        precision = true_positives / k if k > 0 else 0\n",
    "        recall = true_positives / len(actual_top_k) if len(actual_top_k) > 0 else 0\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "    \n",
    "    # Return average precision and recall across all songs\n",
    "    avg_precision = total_precision / num_songs if num_songs > 0 else 0\n",
    "    avg_recall = total_recall / num_songs if num_songs > 0 else 0\n",
    "    \n",
    "    return (avg_precision, avg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c43b1",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c657c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "precision_recall_predict_music_all(1)\n",
    "# return (precision, recall) averaging favorite song of every user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366db97",
   "metadata": {},
   "source": [
    "## Important\n",
    "The example above takes a unreasonable amout of time to finish for dataset size of 1 million. Tested with dataset of 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51465aee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c0f02a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
